{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 782\n",
    "np.random.seed(seed)\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df= pd.read_csv(\"./datasets/train.csv\")\n",
    "train = train_df.values\n",
    "test = pd.read_csv(\"./datasets/test.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[:, 1:]\n",
    "y_train = train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(x, mu, sigma):\n",
    "    \n",
    "    x_norm = np.zeros_like(x)\n",
    "\n",
    "    for n in range(len(x)):\n",
    "        for j in range(len(x[n])):\n",
    "            if(sigma[j]!=0):\n",
    "                x_norm[n,j] = (x[n,j] - mu[j]) / sigma[j]\n",
    "            else:\n",
    "                x_norm[n,j] = 0\n",
    "                    \n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X_train, axis = 0)\n",
    "sigma = np.max(X_train, axis=0) - np.min(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = normalization(test, mu, sigma)\n",
    "normalizer = Normalizer(norm='max')\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "test = normalizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(y_train).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def Relu(x, derivative=False):\n",
    "    if derivative == False:\n",
    "        return x * (x > 0)\n",
    "    else:\n",
    "        return 1 * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    x -= np.max(x)\n",
    "    sm = (np.exp(x).T / np.sum(np.exp(x),axis=1)).T\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWeightAndbiases():\n",
    "    n_inputs = 28 * 28\n",
    "    hidden1 = 30\n",
    "    n_outputs = 10\n",
    "    \n",
    "    # layer1\n",
    "    w1 = np.random.normal(0, n_inputs ** -0.5, [n_inputs, hidden1])\n",
    "    b1 = np.random.normal(0, n_inputs ** -0.5, [1, hidden1])\n",
    "    \n",
    "    w2 = np.random.normal(0, hidden1 ** -0.5, [hidden1, n_outputs])\n",
    "    b2 = np.random.normal(0, hidden1 ** -0.5, [1, n_outputs])\n",
    "    \n",
    "    return [w1, w2, b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dorpout\n",
    "def Dropout(x, dropout_percent):\n",
    "    data = [np.ones_like(x)]\n",
    "    mask = np.random.binomial( data, (1 - dropout_percent) )[0] / (1 - dropout_percent)  \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict(weights, X, dropout_percent=0):\n",
    "    w1, w2, b1, b2 = weights\n",
    "    \n",
    "    first = Relu(np.dot(x, w1) + b1)\n",
    "    \n",
    "    return [first, Softmax(np.dot(first, w2) + b2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 衡量精确度\n",
    "def accuracy(output, y):\n",
    "    hit = 0\n",
    "    output = np.argmax(output, axis = 1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    for x, y in zip(output, y):\n",
    "        if(x == y):\n",
    "            hit += 1\n",
    "    p = (hit * 100) / output.shape[0]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    if x != 0:\n",
    "        return np.log(x)\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "def log(y):\n",
    "    return [[log2(nx) for nx in x] for x in y]\n",
    "\n",
    "def cost(Y_predict, Y_right, weights, nabla):\n",
    "    w1, w2, b1, b2 = weights\n",
    "    weights_sum_square = np.mean(w1 ** 2) + np.mean(w2 ** 2)\n",
    "    Loss = -np.mean(Y_right * log(Y_predict) + (1 - Y_right) * log(1 - Y_predict) + nabla / 2 * weights_sum_square)\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:3.9963178148149465\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 1 1 1 0 0 0 1]\n",
      "86 + 27 = 255\n",
      "------------\n",
      "loss:4.234053567702413\n",
      "Pred:[1 0 1 0 1 0 1 1]\n",
      "True:[1 1 0 1 0 1 0 0]\n",
      "119 + 93 = 171\n",
      "------------\n",
      "loss:3.6456427813137875\n",
      "Pred:[1 1 1 0 0 0 1 1]\n",
      "True:[1 0 1 1 0 0 1 0]\n",
      "121 + 57 = 227\n",
      "------------\n",
      "loss:2.6733470877103773\n",
      "Pred:[0 1 0 1 1 1 1 0]\n",
      "True:[0 1 0 1 1 1 1 0]\n",
      "63 + 31 = 94\n",
      "------------\n",
      "loss:1.6113645322097327\n",
      "Pred:[1 0 0 0 0 0 1 1]\n",
      "True:[1 0 0 0 0 0 1 1]\n",
      "49 + 82 = 131\n",
      "------------\n",
      "loss:0.48996264820318886\n",
      "Pred:[0 1 1 1 1 1 1 1]\n",
      "True:[0 1 1 1 1 1 1 1]\n",
      "84 + 43 = 127\n",
      "------------\n",
      "loss:0.5103971606538684\n",
      "Pred:[1 0 0 1 1 1 0 0]\n",
      "True:[1 0 0 1 1 1 0 0]\n",
      "37 + 119 = 156\n",
      "------------\n",
      "loss:0.39572893953758626\n",
      "Pred:[1 0 0 1 0 1 0 0]\n",
      "True:[1 0 0 1 0 1 0 0]\n",
      "100 + 48 = 148\n",
      "------------\n",
      "loss:0.49936533673331374\n",
      "Pred:[1 1 0 0 1 1 0 0]\n",
      "True:[1 1 0 0 1 1 0 0]\n",
      "79 + 125 = 204\n",
      "------------\n",
      "loss:0.37111513571270416\n",
      "Pred:[0 1 1 0 0 0 0 1]\n",
      "True:[0 1 1 0 0 0 0 1]\n",
      "70 + 27 = 97\n",
      "------------\n",
      "loss:0.3093858794284285\n",
      "Pred:[0 1 0 0 1 0 1 1]\n",
      "True:[0 1 0 0 1 0 1 1]\n",
      "32 + 43 = 75\n",
      "------------\n",
      "loss:0.3538780605145665\n",
      "Pred:[1 1 0 1 1 0 1 0]\n",
      "True:[1 1 0 1 1 0 1 0]\n",
      "126 + 92 = 218\n",
      "------------\n",
      "loss:0.3229951457177708\n",
      "Pred:[1 0 1 1 0 1 1 0]\n",
      "True:[1 0 1 1 0 1 1 0]\n",
      "61 + 121 = 182\n",
      "------------\n",
      "loss:0.1163285786555736\n",
      "Pred:[0 1 0 1 0 1 0 1]\n",
      "True:[0 1 0 1 0 1 0 1]\n",
      "68 + 17 = 85\n",
      "------------\n",
      "loss:0.21076055410497985\n",
      "Pred:[0 1 0 0 1 1 0 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "21 + 56 = 77\n",
      "------------\n",
      "loss:0.1951643991774786\n",
      "Pred:[1 1 1 0 0 1 0 1]\n",
      "True:[1 1 1 0 0 1 0 1]\n",
      "108 + 121 = 229\n",
      "------------\n",
      "loss:0.2358834874656056\n",
      "Pred:[1 1 0 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "69 + 125 = 194\n",
      "------------\n",
      "loss:0.1322899344270084\n",
      "Pred:[0 1 1 0 1 0 1 0]\n",
      "True:[0 1 1 0 1 0 1 0]\n",
      "80 + 26 = 106\n",
      "------------\n",
      "loss:0.18018425545769268\n",
      "Pred:[0 1 1 0 0 1 0 0]\n",
      "True:[0 1 1 0 0 1 0 0]\n",
      "10 + 90 = 100\n",
      "------------\n",
      "loss:0.11216661932304511\n",
      "Pred:[0 1 0 1 1 1 0 1]\n",
      "True:[0 1 0 1 1 1 0 1]\n",
      "21 + 72 = 93\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# sigmoid导数\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# 生成整数与二进制数转化字典\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2, binary_dim)\n",
    "binary = np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T,\n",
    "                       axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "# 模型参数\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "learing_rate = 1e-1\n",
    "\n",
    "# 初始化模型参数\n",
    "# 模型: h(t) = sigmoid(Ux + Vh(t-1)) -> output(t) = sigmoid(Wh(t))\n",
    "U = np.random.randn(input_dim, hidden_dim)\n",
    "V = np.random.randn(hidden_dim, hidden_dim)\n",
    "W = np.random.randn(hidden_dim, output_dim)\n",
    "\n",
    "# 初始化参数梯度\n",
    "dU = np.zeros_like(U)\n",
    "dV = np.zeros_like(V)\n",
    "dW = np.zeros_like(W)\n",
    "\n",
    "iterations = 20000\n",
    "# 训练过程：不使用batch\n",
    "for i in range(iterations):\n",
    "    # 生成一个简单的加法问题 （a+b = c), a, b 除以2防止c溢出\n",
    "    a_int = np.random.randint(largest_number / 2)\n",
    "    a = int2binary[a_int]\n",
    "    b_int = np.random.randint(largest_number / 2)\n",
    "    b = int2binary[b_int]\n",
    "\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "\n",
    "    d = np.zeros_like(c)\n",
    "    # 训练样本\n",
    "    X = np.array([a, b]).T\n",
    "    y = np.array([c]).T\n",
    "\n",
    "    loss = 0  # 损失函数\n",
    "\n",
    "    hs = []  # 保存每个时间步长下的隐含特征\n",
    "    hs.append(np.zeros((1, hidden_dim)))  # 初始化0时刻特征为0\n",
    "    os = []  # 保存每个时间步长的预测值\n",
    "\n",
    "    # forward过程\n",
    "    for t in range(binary_dim):\n",
    "        # 当前时刻特征\n",
    "        xt = X[binary_dim - t - 1]\n",
    "        # 隐含层\n",
    "        ht = sigmoid(xt.dot(U) + hs[-1].dot(V))\n",
    "        # 输出层\n",
    "        ot = sigmoid(ht.dot(W))\n",
    "        # 存储结果\n",
    "        hs.append(ht)\n",
    "        os.append(ot)\n",
    "        # 计算loss，采用L1\n",
    "        loss += np.abs(ot - y[binary_dim - t - 1])[0][0]\n",
    "        # 预测值\n",
    "        d[binary_dim - t - 1] = np.round(ot)[0][0]\n",
    "\n",
    "    # backward过程\n",
    "    future_d_ht = np.zeros((1, hidden_dim))  # 从上一个时刻传递的梯度\n",
    "    for t in reversed(range(binary_dim)):\n",
    "        xt = X[binary_dim - t - 1].reshape(1, -1)\n",
    "        ht = hs[t+1]\n",
    "        ht_prev = hs[t]\n",
    "        ot = os[t]\n",
    "        # d_loss/d_ot\n",
    "        d_ot = ot - y[binary_dim - t - 1]\n",
    "        d_ot_output = sigmoid_derivative(ot) * d_ot\n",
    "        dW += ht.T.dot(d_ot_output)\n",
    "        d_ht = d_ot_output.dot(W.T) + future_d_ht  # 别忘来了上一时刻传入的梯度\n",
    "        d_ht_output = sigmoid_derivative(ht) * d_ht\n",
    "        dU += xt.T.dot(d_ht_output)\n",
    "        dV += ht_prev.T.dot(d_ht_output)\n",
    "\n",
    "        # 更新future_d_ht\n",
    "        future_d_ht = d_ht_output.dot(V.T)\n",
    "\n",
    "    # SGD更新参数\n",
    "    U -= learing_rate * dU\n",
    "    V -= learing_rate * dV\n",
    "    W -= learing_rate * dW\n",
    "\n",
    "    # 重置梯度\n",
    "    dU *= 0\n",
    "    dV *= 0\n",
    "    dW *= 0\n",
    "\n",
    "    # 输出loss和预测结果\n",
    "    if (i % 1000 == 0):\n",
    "        print(\"loss:\" + str(loss))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
