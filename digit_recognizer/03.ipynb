{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上述函数，其输入是包含1个多个枚举类别的2D数组，需要reshape成为这种数组\n",
    "# from sklearn.preprocessing import CategoricalEncoder  #后面会添加这个方法\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# 后面再去理解\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv(\"./datasets/train.csv\")\n",
    "test_df = pd.read_csv(\"./datasets/test.csv\")\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop([\"label\"], axis=1)\n",
    "y_train = train_df[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = CategoricalEncoder(encoding='onehot-dense')\n",
    "y_train_encoder = cat_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_encoder = cat_encoder.transform(y_test.reshape(-1, 1))\n",
    "X_train_small = X_train.values[:100]\n",
    "y_train_small = y_train_encoder[:100]\n",
    "\n",
    "X_test_small = X_test.values[:30]\n",
    "y_test_small = y_test_encoder[:30]\n",
    "training_data = zip(X_train_small, y_train_small)\n",
    "test_data = zip(X_test_small, y_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络的结构\n",
    "class Network():\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k: k + mini_batch_size] for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "#             if test_data:\n",
    "#                 print(\"Epochs {}: {} / {}\".format(j, self.evaluate(test_data), n_test))\n",
    "#             else:\n",
    "#                 print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "#     def evaluate(self, test_data):\n",
    "#         test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "#         return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "#     def feedforward(self, a):\n",
    "#         for b, w in zip(self.biases, self.weights):\n",
    "#             a = sigmoid(np.dot(w, a) + b)\n",
    "#         return a\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # 反向传播\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-1] = delta\n",
    "            nabla_w[-1] = np.dot(delta, activations[-l - 1].transpose())\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        print(y)\n",
    "        print(output_activations)\n",
    "        return (output_activations - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n",
      "10 10\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "[[3.66222227e-01 1.47193472e-01 1.47193472e-01 1.47193472e-01\n",
      "  1.47193472e-01 3.66222227e-01 1.47193472e-01 3.66222227e-01\n",
      "  1.47193472e-01 1.47193472e-01 3.66222227e-01 3.66222227e-01\n",
      "  1.47193472e-01 3.66222227e-01 1.47193472e-01 1.47193472e-01\n",
      "  1.47193472e-01 3.66222227e-01 3.66222227e-01 3.66222227e-01\n",
      "  1.47193472e-01 3.66222227e-01 3.66222227e-01 1.47193472e-01\n",
      "  1.47193472e-01 1.47193472e-01 1.47193472e-01 3.66222227e-01\n",
      "  3.66222227e-01 3.66222227e-01]\n",
      " [9.04252270e-01 2.15766765e-02 2.15766765e-02 2.15766765e-02\n",
      "  2.15766765e-02 9.04252270e-01 2.15766765e-02 9.04252270e-01\n",
      "  2.15766765e-02 2.15766765e-02 9.04252270e-01 9.04252270e-01\n",
      "  2.15766765e-02 9.04252270e-01 2.15766765e-02 2.15766765e-02\n",
      "  2.15766765e-02 9.04252270e-01 9.04252270e-01 9.04252270e-01\n",
      "  2.15766765e-02 9.04252270e-01 9.04252270e-01 2.15766765e-02\n",
      "  2.15766765e-02 2.15766765e-02 2.15766765e-02 9.04252270e-01\n",
      "  9.04252270e-01 9.04252270e-01]\n",
      " [7.36916378e-01 8.87097394e-02 8.87097394e-02 8.87097394e-02\n",
      "  8.87097394e-02 7.36916378e-01 8.87097394e-02 7.36916378e-01\n",
      "  8.87097394e-02 8.87097394e-02 7.36916378e-01 7.36916378e-01\n",
      "  8.87097394e-02 7.36916378e-01 8.87097394e-02 8.87097394e-02\n",
      "  8.87097394e-02 7.36916378e-01 7.36916378e-01 7.36916378e-01\n",
      "  8.87097394e-02 7.36916378e-01 7.36916378e-01 8.87097394e-02\n",
      "  8.87097394e-02 8.87097394e-02 8.87097394e-02 7.36916378e-01\n",
      "  7.36916378e-01 7.36916378e-01]\n",
      " [3.19693944e-01 4.48868427e-01 4.48868427e-01 4.48868427e-01\n",
      "  4.48868427e-01 3.19693944e-01 4.48868427e-01 3.19693944e-01\n",
      "  4.48868427e-01 4.48868427e-01 3.19693944e-01 3.19693944e-01\n",
      "  4.48868427e-01 3.19693944e-01 4.48868427e-01 4.48868427e-01\n",
      "  4.48868427e-01 3.19693944e-01 3.19693944e-01 3.19693944e-01\n",
      "  4.48868427e-01 3.19693944e-01 3.19693944e-01 4.48868427e-01\n",
      "  4.48868427e-01 4.48868427e-01 4.48868427e-01 3.19693944e-01\n",
      "  3.19693944e-01 3.19693944e-01]\n",
      " [4.44247935e-01 7.81709598e-01 7.81709598e-01 7.81709598e-01\n",
      "  7.81709598e-01 4.44247935e-01 7.81709598e-01 4.44247935e-01\n",
      "  7.81709598e-01 7.81709598e-01 4.44247935e-01 4.44247935e-01\n",
      "  7.81709598e-01 4.44247935e-01 7.81709598e-01 7.81709598e-01\n",
      "  7.81709598e-01 4.44247935e-01 4.44247935e-01 4.44247935e-01\n",
      "  7.81709598e-01 4.44247935e-01 4.44247935e-01 7.81709598e-01\n",
      "  7.81709598e-01 7.81709598e-01 7.81709598e-01 4.44247935e-01\n",
      "  4.44247935e-01 4.44247935e-01]\n",
      " [3.35902063e-01 2.17340850e-04 2.17340850e-04 2.17340850e-04\n",
      "  2.17340850e-04 3.35902063e-01 2.17340850e-04 3.35902063e-01\n",
      "  2.17340850e-04 2.17340850e-04 3.35902063e-01 3.35902063e-01\n",
      "  2.17340850e-04 3.35902063e-01 2.17340850e-04 2.17340850e-04\n",
      "  2.17340850e-04 3.35902063e-01 3.35902063e-01 3.35902063e-01\n",
      "  2.17340850e-04 3.35902063e-01 3.35902063e-01 2.17340850e-04\n",
      "  2.17340850e-04 2.17340850e-04 2.17340850e-04 3.35902063e-01\n",
      "  3.35902063e-01 3.35902063e-01]\n",
      " [6.61203426e-01 4.93803704e-01 4.93803704e-01 4.93803704e-01\n",
      "  4.93803704e-01 6.61203426e-01 4.93803704e-01 6.61203426e-01\n",
      "  4.93803704e-01 4.93803704e-01 6.61203426e-01 6.61203426e-01\n",
      "  4.93803704e-01 6.61203426e-01 4.93803704e-01 4.93803704e-01\n",
      "  4.93803704e-01 6.61203426e-01 6.61203426e-01 6.61203426e-01\n",
      "  4.93803704e-01 6.61203426e-01 6.61203426e-01 4.93803704e-01\n",
      "  4.93803704e-01 4.93803704e-01 4.93803704e-01 6.61203426e-01\n",
      "  6.61203426e-01 6.61203426e-01]\n",
      " [5.10282544e-01 8.74004329e-01 8.74004329e-01 8.74004329e-01\n",
      "  8.74004329e-01 5.10282544e-01 8.74004329e-01 5.10282544e-01\n",
      "  8.74004329e-01 8.74004329e-01 5.10282544e-01 5.10282544e-01\n",
      "  8.74004329e-01 5.10282544e-01 8.74004329e-01 8.74004329e-01\n",
      "  8.74004329e-01 5.10282544e-01 5.10282544e-01 5.10282544e-01\n",
      "  8.74004329e-01 5.10282544e-01 5.10282544e-01 8.74004329e-01\n",
      "  8.74004329e-01 8.74004329e-01 8.74004329e-01 5.10282544e-01\n",
      "  5.10282544e-01 5.10282544e-01]\n",
      " [6.24553467e-01 9.27272425e-01 9.27272425e-01 9.27272425e-01\n",
      "  9.27272425e-01 6.24553467e-01 9.27272425e-01 6.24553467e-01\n",
      "  9.27272425e-01 9.27272425e-01 6.24553467e-01 6.24553467e-01\n",
      "  9.27272425e-01 6.24553467e-01 9.27272425e-01 9.27272425e-01\n",
      "  9.27272425e-01 6.24553467e-01 6.24553467e-01 6.24553467e-01\n",
      "  9.27272425e-01 6.24553467e-01 6.24553467e-01 9.27272425e-01\n",
      "  9.27272425e-01 9.27272425e-01 9.27272425e-01 6.24553467e-01\n",
      "  6.24553467e-01 6.24553467e-01]\n",
      " [3.19838798e-01 1.74993218e-01 1.74993218e-01 1.74993218e-01\n",
      "  1.74993218e-01 3.19838798e-01 1.74993218e-01 3.19838798e-01\n",
      "  1.74993218e-01 1.74993218e-01 3.19838798e-01 3.19838798e-01\n",
      "  1.74993218e-01 3.19838798e-01 1.74993218e-01 1.74993218e-01\n",
      "  1.74993218e-01 3.19838798e-01 3.19838798e-01 3.19838798e-01\n",
      "  1.74993218e-01 3.19838798e-01 3.19838798e-01 1.74993218e-01\n",
      "  1.74993218e-01 1.74993218e-01 1.74993218e-01 3.19838798e-01\n",
      "  3.19838798e-01 3.19838798e-01]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,30) (10,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-0e60b6771afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnetwor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-210-e51b5029bb8a>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     20\u001b[0m             ]\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#             if test_data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#                 print(\"Epochs {}: {} / {}\".format(j, self.evaluate(test_data), n_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-e51b5029bb8a>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, eta)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdnw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-e51b5029bb8a>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mnabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-210-e51b5029bb8a>\u001b[0m in \u001b[0;36mcost_derivative\u001b[0;34m(self, output_activations, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_activations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_activations\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,30) (10,) "
     ]
    }
   ],
   "source": [
    "sizes = [784, 30, 10]\n",
    "networ = Network(sizes)\n",
    "networ.SGD(training_data, 30, 10, 3.0, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 784)\n",
      "(10, 30)\n"
     ]
    }
   ],
   "source": [
    "for a in network.weights:\n",
    "    print(np.array(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
